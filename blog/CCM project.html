<!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <title>proposal</title>
            <style>
/* From extension vscode.github */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.vscode-dark img[src$=\#gh-light-mode-only],
.vscode-light img[src$=\#gh-dark-mode-only] {
	display: none;
}

</style>
            
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item {
    list-style-type: none;
}

.task-list-item-checkbox {
    margin-left: -20px;
    vertical-align: middle;
    pointer-events: none;
}
</style>
<style>
:root {
  --color-note: #0969da;
  --color-tip: #1a7f37;
  --color-warning: #9a6700;
  --color-severe: #bc4c00;
  --color-caution: #d1242f;
  --color-important: #8250df;
}

</style>
<style>
@media (prefers-color-scheme: dark) {
  :root {
    --color-note: #2f81f7;
    --color-tip: #3fb950;
    --color-warning: #d29922;
    --color-severe: #db6d28;
    --color-caution: #f85149;
    --color-important: #a371f7;
  }
}

</style>
<style>
.markdown-alert {
  padding: 0.5rem 1rem;
  margin-bottom: 16px;
  color: inherit;
  border-left: .25em solid #888;
}

.markdown-alert>:first-child {
  margin-top: 0
}

.markdown-alert>:last-child {
  margin-bottom: 0
}

.markdown-alert .markdown-alert-title {
  display: flex;
  font-weight: 500;
  align-items: center;
  line-height: 1
}

.markdown-alert .markdown-alert-title .octicon {
  margin-right: 0.5rem;
  display: inline-block;
  overflow: visible !important;
  vertical-align: text-bottom;
  fill: currentColor;
}

.markdown-alert.markdown-alert-note {
  border-left-color: var(--color-note);
}

.markdown-alert.markdown-alert-note .markdown-alert-title {
  color: var(--color-note);
}

.markdown-alert.markdown-alert-important {
  border-left-color: var(--color-important);
}

.markdown-alert.markdown-alert-important .markdown-alert-title {
  color: var(--color-important);
}

.markdown-alert.markdown-alert-warning {
  border-left-color: var(--color-warning);
}

.markdown-alert.markdown-alert-warning .markdown-alert-title {
  color: var(--color-warning);
}

.markdown-alert.markdown-alert-tip {
  border-left-color: var(--color-tip);
}

.markdown-alert.markdown-alert-tip .markdown-alert-title {
  color: var(--color-tip);
}

.markdown-alert.markdown-alert-caution {
  border-left-color: var(--color-caution);
}

.markdown-alert.markdown-alert-caution .markdown-alert-title {
  color: var(--color-caution);
}

</style>
        
        </head>
        <body class="vscode-body vscode-light">
            <p>brain simulation</p>
<h2 id="proposal">proposal</h2>
<p>We plan to build a human connectome-constrained artificial neural network(ANN).
\</p>
<p>Each brain region will serve as a node in a graph-like neural network and each node will be a multi-layer convolutional neural network(Exploring Randomly Wired Neural Networks for Image Recognition <a href="https://arxiv.org/abs/1904.01569">https://arxiv.org/abs/1904.01569</a>). Those nodes will be connected based on the functional connectivity from fMRI or structural connectivity from DWI.
\</p>
<p>We will use the dataset of fMRI and eye gaze recording(A studyforrest extension, simultaneous fMRI and eye gaze recordings during prolonged natural stimulation <a href="https://www.nature.com/articles/sdata201692">https://www.nature.com/articles/sdata201692</a>). The input of ANN will be the video simulation for the participant, and the output will be the attention heatmap for the eye gaze.
\</p>
<p>After the training of the ANN, we want to evaluate the correlation between fMRI activity and node activity. Here, the node activity might be defined as the difference between the input and the output(Generalized Shape Metrics on Neural Representations <a href="https://arxiv.org/abs/2110.14739">https://arxiv.org/abs/2110.14739</a>).</p>
<h2 id="method">method</h2>
<h2 id="code">code:</h2>
<p><strong>!!! the structure</strong><br>
<a href="https://arxiv.org/abs/1904.01569">Exploring Randomly Wired Neural Networks for Image Recognition</a><br>
Saining Xie, Alexander Kirillov, Ross Girshick, Kaiming He</p>
<p><strong>!!! the loss function of fMRI</strong><br>
<a href="https://arxiv.org/abs/2401.17231">ReAlnet: Achieving More Human Brain-Like Vision via Human Neural Representational Alignment</a></p>
<h2 id="data">data:</h2>
<p><strong>!!!</strong><br>
<a href="https://www.nature.com/articles/sdata201692">eye gaze and fMRI</a><br>
A studyforrest extension, simultaneous fMRI and eye gaze recordings during prolonged natural stimulation
<a href="https://openfmri.org/dataset/ds000113d/">https://openfmri.org/dataset/ds000113d/</a>
Simultaneous fMRI/eyetracking while movie watching, plus visual localizers</p>
<p><strong>!!!</strong><br>
<a href="https://www.nature.com/articles/sdata20143">audio movie</a>
A high-resolution 7-Tesla fMRI dataset from complex natural stimulation with an audio movie
<a href="https://openfmri.org/dataset/ds000113/">https://openfmri.org/dataset/ds000113/</a></p>
<p>Diffusion-weighted imaging (DWI)
<a href="https://www.studyforrest.org/explore.html">https://www.studyforrest.org/explore.html</a></p>
<p>Multi-resolution 7T fMRI data on the representation of visual orientation
<a href="https://openfmri.org/dataset/ds000113c/">https://openfmri.org/dataset/ds000113c/</a></p>
<p>other eye gaze and fMRI paper:</p>
<p><a href="https://academic.oup.com/cercor/article/30/3/1171/5583730">Evaluating fMRI-Based Estimation of Eye Gaze During Naturalistic Viewing</a></p>
<p><a href="https://academic.oup.com/scan/article/6/4/393/1649259">Eyes on me: an fMRI study of the effects of social gaze on action control</a></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1053811910014631">Processing social aspects of human gaze: A combined fMRI-DTI study</a></p>
<h2 id="reference">Reference</h2>
<p><a href="https://lengyuner.github.io/blog/connectome%20inspired%20neural%20network.html">Connectome inspired neural network</a></p>
<p><a href="https://elifesciences.org/articles/67400">The structural connectome constrains fast brain dynamics</a></p>
<p><a href="https://www.nature.com/articles/s41467-018-07471-9">Predicting eye movement patterns from fMRI responses to natural scenes</a></p>
<h2 id="others">others</h2>
<p>saccade</p>
<p><img src="file:////Users/lengyuner/Desktop/lengyuner.github.io/blog/image.png" alt="alt text">
<a href="https://www.nature.com/articles/eye2014284/figures/1">https://www.nature.com/articles/eye2014284/figures/1</a></p>
<p>Superior Colliculus</p>
<p><img src="file:////Users/lengyuner/Desktop/lengyuner.github.io/blog/image-1.png" alt="alt text">
<a href="https://www.sciencedirect.com/science/article/pii/S0959438810000255?via%3Dihub">https://www.sciencedirect.com/science/article/pii/S0959438810000255?via%3Dihub</a></p>
<p>attention map
<a href="https://www.mdpi.com/ijgi/ijgi-10-00664/article_deploy/html/images/ijgi-10-00664-g003.png">https://www.mdpi.com/ijgi/ijgi-10-00664/article_deploy/html/images/ijgi-10-00664-g003.png</a></p>

            
            
        </body>
        </html>